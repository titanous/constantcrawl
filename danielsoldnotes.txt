For the past few months I've been playing with the idea of a distributed crawler/spider.

There are a lot of technical points that should be discussed in far greater detail and different paths to choose from. This document is only intended to give a sense of where my thoughts are right now and give you enough information so you can express interest or a lack thereof. Please have a read and let me know what you think. -D

The geometrically increasing size and iterating speed of the web is getting to big for anyone (except Google) to crawl effectively, and too big for anyone (including Google) to index in anything like real time. Like so many other problems in digital, the solution may be to distribute the work.

A distributed crawler (running as say a browser extension) would produce data proportional to actual user interests (one of the biggest problems in collecting data for search engines). Additionally it's perhaps the only way to "out-Google" Google. No one else will ever be able to afford the hardware and bandwidth necessary to compete with, let alone significantly outpace Google, unless the task is divided into infinitesimal parts and distributed to a massive number of users.

Home users have demonstrated strong interest in distributed computing in the past in mainstream science projects like SETI and Folding @home, file sharing through BitTorrent, and even offsetting commercial bandwidth costs through services like Spotify.

Indexing every page every user visited would also help protect activists and fight against censorship in repressive regimes. Once one person had viewed a site, it wouldn't matter if that site went offline later. Even following a DNS seizure, the cached version of the page would be available to the world. Potentially this would also allow development of alternatives to Certificate Authorities and the centralized DNS system.

Pages experiencing a sudden increase in popularity suffer from a similar problem. A small site that hits the front page of Hacker News or Reddit will likely become inaccessible almost immediately as the tiny server is overloaded or bandwidth caps are unexpectedly reached. The cached version of the site (which is proportionately more likely to exist the more popular the site is and therefor the more likely it is currently unavailable) could be made available as a fallback if the server responds with an error or after a user-specified page load delay.

Obviously small and upstart search engines would be interested in this data (as, one assumes larger engines would be as well). The data collected would not constitute an engine in it's own right, simply the beginnings of an index. Someone creating a search engine would need a complete copy of the data on their own server and an algorithm to sort the results.

The data itself could be made available in two ways:

First as nightly compilations from a central server on a public or private cloud, not unlike CommonCrawl provides today, but far larger in scale. The costs for such a service would be significant and hosting would need to be selected with great care. Ideally there would be not one single destination for the complete collection, and many different providers would make the data available. Amazon or Rackspace might offer to host some or all for free, but no large corporation should be the sole guardian or provider for information of this sort.

Additionally, a purely p2p model could be implemented, similar to the Spotify backend where a small portion (100MB-1GB) of each user's hard drive is dedicated to caching not only his or her recently scraped pages, but a selection of pages collected from other users. When a user tried to load a page that is currently unavailable, the browser would fall back on this p2p network and find the nearest available reliable copy.

Great caution would be required in designing the sharing algorithms at work here as the information each user collects would amount to their entire personal browsing history, which must be both private and protected. Caches, connections, and uploads should be completely anonymized as a primary design goal.

The distributed sharing approach protects against broad internet outages or censorship when combined with other technologies like TOR and wireless mesh darknets. If an entire country was disconnected from the internet but had a mesh darknet in place and a single user online via satellite data, that user could download regularly and cache a selection of the most requested and immediately necessary pages (news, protest information) which the network could then relay safely to other users in need.

If this system reached any serious level of adoption, a number of new problems would emerge. Interested parties might attempt to seed false caches of sites for any number of reasons. Another significant design challenge would center around verifying certain caches as accurate. Particularly vulnerable pages (activist blogs, impartial news reports) could be verified by whitelisted accounts (EFF, Amnesty International, direct uploads of those news publishers/reporters) and compared against crowdsourced reports to potentially reveal puppet networks and botnets controlled by governments and organized criminal organizations. (This data would be invaluable on its own.) Besides whitelisted servers, there are other algorithms that could compare users' browsing histories to verify a real human was operating the computer, but again, these would need to be considered with the greatest care and attention to protecting users' privacy.

A browser extension seems like the least invasive or disruptive method of deployment and could be spend immensely by the support of some of the major browser developers. However every browser provider has separate motivations (Chrome is Google, IE is Bing, FF gets all its money from Google, Opera already has a suspicious mobile caching operation in place) that could undermine the goals of this project and so their support should be sought and accepted only after this standard has been independently developed. Ideal partners should all have displayed the strongest possible commitments to privacy and protecting the freedom of the internet.
